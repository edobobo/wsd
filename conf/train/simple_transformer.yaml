# reproducibility
seed: 42

# model name
model_name: default_name  # used to name the directory in which model's checkpoints will be stored (experiments/model_name/...)

# pl_trainer
pl_trainer:
  _target_: pytorch_lightning.Trainer
  gpus: 1
  accumulate_grad_batches: 1
  gradient_clip_val: 10.0
  val_check_interval: 2_00
  max_steps: 1_000_000
  precision: 16
  amp_level: O1

# optimization
optim:
  _target_: src.optim.factories.RAdamWithDecayFactory
  lr: 1e-4
  weight_decay: 0.01
  no_decay_params:
    - bias
    - LayerNorm.weight

# LIGHTNING CALLBACKS
callbacks_monitor: val_loss
callbacks_mode: min

# early stopping callback
# "early_stopping_callback: null" will disable early stopping
early_stopping_callback:
  _target_: pytorch_lightning.callbacks.EarlyStopping
  monitor: ${train.callbacks_monitor}
  mode: ${train.callbacks_mode}
  patience: 25

# model_checkpoint_callback
# "model_checkpoint_callback: null" will disable model checkpointing
model_checkpoint_callback:
  # _target_: pytorch_lightning.callbacks.ModelCheckpoint
  _target_: src.pl_callbacks.best_checkpoint.ModelCheckpointWithBest
  monitor: ${train.callbacks_monitor}
  mode: ${train.callbacks_mode}
  verbose: True
  save_top_k: 3
  dirpath: checkpoints


# logging
# TODO: place here comet stuff
